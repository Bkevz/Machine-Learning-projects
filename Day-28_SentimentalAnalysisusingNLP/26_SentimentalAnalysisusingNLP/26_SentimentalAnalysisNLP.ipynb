{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "26_SentimentalAnalysisNLP.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mkgh9SoBSOwH"
      },
      "source": [
        "#26 Sentimental Analysis NLP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UNRNEQQG9Sr"
      },
      "source": [
        "!pip install nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6-o7BjJTzVx"
      },
      "source": [
        "### Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLZbzW6wT2ZQ"
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "import re #Regular expressions\n",
        "import nltk \n",
        "import matplotlib.pyplot as plt \n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkPr6C5_UH9d"
      },
      "source": [
        "### Load Dataset from Local Directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5rC2iqzUJEm"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aK8lU0OUUlX0"
      },
      "source": [
        "### Importing Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2I6egUxUoaq"
      },
      "source": [
        "dataset = pd.read_csv('dataset.csv')\n",
        "print(dataset.shape)\n",
        "print(dataset.head(5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfM237zDUwSx"
      },
      "source": [
        "###Segregating Dataset into Input & Output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ReRaankPU1f0"
      },
      "source": [
        "features = dataset.iloc[:, 10].values\n",
        "labels = dataset.iloc[:, 1].values\n",
        "print(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQwlfdWsj2LT"
      },
      "source": [
        "###Removing the Special Character"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhesmp0CU9xc"
      },
      "source": [
        "processed_features = []\n",
        "\n",
        "for sentence in range(0, len(features)):\n",
        "    # Remove all the special characters\n",
        "    processed_feature = re.sub(r'\\W', ' ', str(features[sentence]))\n",
        "\n",
        "    # remove all single characters\n",
        "    processed_feature= re.sub(r'\\s+[a-zA-Z]\\s+', ' ', processed_feature)\n",
        "\n",
        "    # Remove single characters from the start\n",
        "    processed_feature = re.sub(r'\\^[a-zA-Z]\\s+', ' ', processed_feature) \n",
        "\n",
        "    # Substituting multiple spaces with single space\n",
        "    processed_feature = re.sub(r'\\s+', ' ', processed_feature, flags=re.I)\n",
        "\n",
        "    # Removing prefixed 'b'\n",
        "    processed_feature = re.sub(r'^b\\s+', '', processed_feature)\n",
        "\n",
        "    # Converting to Lowercase\n",
        "    processed_feature = processed_feature.lower()\n",
        "\n",
        "    processed_features.append(processed_feature)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "meD0mcOVj5rK"
      },
      "source": [
        "###Feature Extraction from text\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzisF0taVA_b"
      },
      "source": [
        "nltk.download('stopwords')\n",
        "vectorizer = TfidfVectorizer (max_features=2500, min_df=7, max_df=0.8, stop_words=stopwords.words('english'))\n",
        "processed_features = vectorizer.fit_transform(processed_features).toarray()\n",
        "print(processed_features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2vFlF_fj-SK"
      },
      "source": [
        "###Splitting Dataset into Train & Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4fpb6RmVI0t"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(processed_features, labels, test_size=0.2, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9zzG3lDkC9L"
      },
      "source": [
        "###Loading Random Forest Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0TG77VbjVK7H"
      },
      "source": [
        "text_classifier = RandomForestClassifier(n_estimators=200, random_state=0)\n",
        "text_classifier.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-cE4paSkH69"
      },
      "source": [
        "###Predicting the Test data with Trained Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JjNeO6rQVMfr"
      },
      "source": [
        "predictions = text_classifier.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzNioePqkMnH"
      },
      "source": [
        "###Score of the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2TY6JdyVOXn"
      },
      "source": [
        "print(accuracy_score(y_test, predictions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGL1jroOkRaR"
      },
      "source": [
        "###Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Skyz1_qpVQgl"
      },
      "source": [
        "from sklearn import metrics\n",
        "import itertools\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "\n",
        "cm = metrics.confusion_matrix(y_test, predictions, labels=['negative', 'neutral', 'positive'])\n",
        "plot_confusion_matrix(cm, classes=['negative', 'neutral', 'positive'])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}